[
  {
    "slug": "create-test-plans",
    "name": "Create Test Plans",
    "description": "Generates structured test plans from requirements or user stories, producing test cases with steps and expected results.",
    "category": "testing",
    "version": "1.0.0",
    "inputs": [
      {
        "name": "requirements",
        "type": "string",
        "required": true,
        "description": "The requirements, user stories, or feature specification to generate test plans for."
      },
      {
        "name": "format",
        "type": "string",
        "required": false,
        "default": "markdown",
        "description": "Output format for the test plan: 'markdown' or 'json'."
      }
    ],
    "prompt": "You are a senior QA engineer creating a structured test plan.\n\n## Requirements\n{{requirements}}\n\n## Output Format Preference\n{{format}}\n\n## Instructions\nAnalyze the requirements above and produce a comprehensive test plan. For each requirement or user story, generate test cases that cover:\n\n1. **Happy path** — the expected successful flow\n2. **Edge cases** — boundary values, empty inputs, maximum lengths\n3. **Error scenarios** — invalid inputs, network failures, permission errors\n4. **Integration points** — interactions with other components or services\n\n## Output Structure\nProduce a structured test plan with the following sections:\n\n### Test Plan Summary\n- Total number of test cases\n- Coverage areas\n- Risk assessment\n\n### Test Cases\nFor each test case, include:\n- **ID**: TC-XXX (sequential numbering)\n- **Title**: Concise description of what is being tested\n- **Preconditions**: Setup required before executing the test\n- **Steps**: Numbered steps to execute the test\n- **Expected Result**: What should happen when the test passes\n- **Priority**: High / Medium / Low\n- **Category**: Functional / Integration / Edge Case / Error Handling\n\n### Test Data Requirements\nList any test data or fixtures needed.\n\n### Automation Notes\nSuggest which test cases are good candidates for automation and which should remain manual."
  },
  {
    "slug": "auto-pass-tests",
    "name": "Automatically Pass All Tests",
    "description": "Analyzes failing tests and applies fixes, mocks, or configuration changes to make them pass.",
    "category": "testing",
    "version": "1.0.0",
    "inputs": [
      {
        "name": "testSuite",
        "type": "string",
        "required": true,
        "description": "Test suite identifier, file path, or glob pattern for the tests to fix (e.g., 'src/__tests__/*.test.ts' or 'npm test')."
      },
      {
        "name": "testCommand",
        "type": "string",
        "required": false,
        "default": "npm test",
        "description": "The command used to run the test suite."
      }
    ],
    "prompt": "You are a senior software engineer tasked with making all failing tests pass.\n\n## Test Suite\n{{testSuite}}\n\n## Test Command\n{{testCommand}}\n\n## Instructions\nFollow these steps precisely:\n\n1. **Run the test suite** using the test command to identify all failing tests.\n2. **Analyze each failure** — read the test file and the source code it tests. Understand what the test expects vs what actually happens.\n3. **Determine the root cause** for each failure:\n   - Is the source code buggy? → Fix the source code.\n   - Is the test outdated or incorrect? → Update the test to match current behavior (only if the current behavior is correct).\n   - Are mocks missing or misconfigured? → Add or fix mocks.\n   - Are dependencies or configurations missing? → Add them.\n4. **Apply the minimal fix** for each failure. Do not refactor unrelated code.\n5. **Re-run the test suite** after each fix to verify it passes.\n6. **Repeat** until all tests pass.\n\n## Rules\n- Never delete or skip tests to make the suite pass.\n- Prefer fixing source code over changing test expectations.\n- If a test is genuinely wrong (testing incorrect behavior), document why before changing it.\n- Keep fixes minimal and focused — one concern per change.\n- Run the full suite after all fixes to catch regressions.\n\n## Output\nAfter all tests pass, produce a summary:\n- Total tests fixed\n- For each fix: file changed, what was wrong, what was fixed\n- Any remaining warnings or concerns"
  },
  {
    "slug": "install-plugins",
    "name": "Install Plugins",
    "description": "Automates discovery, download, and installation of plugins from a registry or local source.",
    "category": "devops",
    "version": "1.0.0",
    "inputs": [
      {
        "name": "plugins",
        "type": "string",
        "required": true,
        "description": "Plugin name or comma-separated list of plugin names to install."
      },
      {
        "name": "registry",
        "type": "string",
        "required": false,
        "default": "npm",
        "description": "Plugin registry source: 'npm', 'pip', 'vscode', or a custom registry URL."
      }
    ],
    "prompt": "You are a DevOps engineer installing plugins/packages for a project.\n\n## Plugins to Install\n{{plugins}}\n\n## Registry\n{{registry}}\n\n## Instructions\nFor each plugin in the list:\n\n1. **Validate** — Check if the plugin name is valid and exists in the registry.\n2. **Check compatibility** — Verify the plugin is compatible with the current project (check engine requirements, peer dependencies, version constraints).\n3. **Resolve dependencies** — Identify all transitive dependencies and check for conflicts with existing packages.\n4. **Install** — Run the appropriate install command for the registry:\n   - npm: `npm install <package>`\n   - pip: `pip install <package>`\n   - vscode: `code --install-extension <extension-id>`\n   - Custom URL: Download and install from the provided URL\n5. **Verify** — After installation, verify the plugin loads correctly:\n   - Check that the package appears in the lock file / dependency list\n   - Run a basic smoke test if available\n   - Check for any deprecation warnings\n6. **Rollback on failure** — If installation fails, revert any partial changes and report the error.\n\n## Rules\n- Install plugins one at a time to isolate failures.\n- Always use exact versions or lock files to ensure reproducibility.\n- Do not modify existing plugin configurations unless required for compatibility.\n- Report any security advisories for installed packages.\n\n## Output\nProduce an installation report:\n- For each plugin: name, version installed, dependencies added, status (success/failed)\n- Any warnings or security advisories\n- Post-installation steps required (if any)"
  }
]
